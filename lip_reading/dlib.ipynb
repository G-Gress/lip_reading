{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db751c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import imageio\n",
    "from imutils import face_utils\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b0d40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 40 video files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 4877.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Lip extraction complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Paths\n",
    "video_dir = \"raw_data/videos/s1\"\n",
    "alignment_dir = \"raw_data/alignments/s1\"\n",
    "output_dir = \"lip_reading/processed_lips\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Load dlib models\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor =(\"raw_data/videos/s1.mpg_vcd/s1/bbaf2n.mpg\")\n",
    "\n",
    "# Lip landmark indices (mouth)\n",
    "LIP_INDEXES = list(range(48, 68))\n",
    "\n",
    "def extract_lip_region(frame, landmarks):\n",
    "    lip_points = landmarks[LIP_INDEXES]\n",
    "    x, y, w, h = cv2.boundingRect(np.array(lip_points))\n",
    "    margin = 10\n",
    "    x = max(x - margin, 0)\n",
    "    y = max(y - margin, 0)\n",
    "    return frame[y:y+h+margin, x:x+w+margin]\n",
    "\n",
    "# Get all video files\n",
    "video_files = \"raw_data/videos/s1.mpg_vcd/s1/bbaf2n.mpg\"\n",
    "\n",
    "print(f\"Processing {len(video_files)} video files...\")\n",
    "\n",
    "for video_file in tqdm(video_files):\n",
    "    video_path = os.path.join(video_dir, video_file)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    frame_num = 0\n",
    "    success = True\n",
    "    while success:\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = detector(gray)\n",
    "\n",
    "        if len(faces) > 0:\n",
    "            shape = predictor(gray, faces[0])\n",
    "            shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "            lip = extract_lip_region(frame, shape)\n",
    "\n",
    "            # Save the lip image\n",
    "            lip_filename = f\"{video_file[:-4]}_frame{frame_num}.png\"\n",
    "            lip_path = os.path.join(output_dir, lip_filename)\n",
    "            cv2.imwrite(lip_path, lip)\n",
    "\n",
    "        frame_num += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "print(\"✅ Lip extraction complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb67241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 40 video files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 1404.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Lip extraction complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#  Set your video and alignment directories\n",
    "video_dir = \"raw_data/videos/s1.mpg_vcd/s1\"  # <-- Set to directory, not file\n",
    "alignment_dir = \"/home/diya871/code/G-Gress/lip_reading/raw_data/alignments /s1/align/bbaf2n.align\"\n",
    "output_dir = \"lip_reading/processed_lips\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "#  Lip landmark indices from dlib's 68-point face model\n",
    "LIP_INDEXES = list(range(48, 68))\n",
    "\n",
    "def extract_lip_region(frame, landmarks):\n",
    "    \"\"\"Crop the lip region based on 68-point landmarks\"\"\"\n",
    "    lip_points = landmarks[LIP_INDEXES]\n",
    "    x, y, w, h = cv2.boundingRect(np.array(lip_points))\n",
    "    margin = 10  # Add padding around lips\n",
    "    x = max(x - margin, 0)\n",
    "    y = max(y - margin, 0)\n",
    "    return frame[y:y+h+margin, x:x+w+margin]\n",
    "\n",
    "\n",
    "print(f\"Processing {len(video_files)} video files...\")\n",
    "\n",
    "for video_file in tqdm(video_files):\n",
    "    video_path = os.path.join(video_dir, video_file)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    frame_num = 0\n",
    "    success = True\n",
    "    while success:\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = detector(gray)\n",
    "\n",
    "        if len(faces) > 0:\n",
    "            shape = predictor(gray, faces[0])\n",
    "            shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "            lip = extract_lip_region(frame, shape)\n",
    "\n",
    "            #  Save the cropped lip image\n",
    "            lip_filename = f\"{video_file[:-4]}_frame{frame_num}.png\"\n",
    "            lip_path = os.path.join(output_dir, lip_filename)\n",
    "            cv2.imwrite(lip_path, lip)\n",
    "\n",
    "        frame_num += 1\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "print(\"✅ Lip extraction complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a7aa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "print(\"Extracted lip images:\")\n",
    "for fname in os.listdir(output_dir):\n",
    "    print(fname)\n",
    "    display(Image(filename=os.path.join(output_dir, fname)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69b7f3bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: (288, 360, 3)\n"
     ]
    }
   ],
   "source": [
    "# Ensure cv2 is imported\n",
    "import cv2\n",
    "\n",
    "# Get input shape (height, width, channels) of the first video file\n",
    "first_video = (\"/home/diya871/code/G-Gress/lip_reading/raw_data/videos/s1.mpg_vcd/s1/bbaf2n.mpg\")\n",
    "video_path = first_video  # Use the absolute path directly\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "ret, frame = cap.read()\n",
    "if ret:\n",
    "    print(\"Input shape:\", frame.shape)  # (height, width, channels)\n",
    "else:\n",
    "    print(\"Failed to read video.\")\n",
    "cap.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lip_reading-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
